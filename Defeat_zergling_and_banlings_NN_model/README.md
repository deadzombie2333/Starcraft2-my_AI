# Mini game, defeat zerglings and banglings:

In mini game Defeat zerglings and banelings, friendly units are terran marines while hostile unit are zerg zerglings and banelings. Based on knowledge, banelings are melee (short range) unit which have higher damage to group of light armor unit such as marines (one can also observe this by play this mini game for several rounds). Therefore, it is important to select the optimal attacking target of friendly unit and take advantage of attack range of marines. A full of AI architecture of this mini game tactic would be first acknowledge the advantage of attack range of friendly unit with disadvantage of attack damage (DPS). Then, based on this, a “hit and run” tactic is designed to limit hostile unit damage (only part of hostile unit can attack friendly unit at the same time). In my project, I bypassed the first decision part and went straight to marine “hit and run” micro. 

# AI architecture:

In my AI design, two design questions are considered, one is what unit to attack when attacking, and another is where to move when moving. Two neural networks are designed to answer this question. 
When attacking, I attempt to design a NN that calculate the highest threat of all enemy unit toward friendly unit and the highest threat unit should be eliminated first. In my design, I take each friendly-hostile unit as one pair, input both unit damage per second versus the other, distance between each other and the health of friendly unit as input for my NN. NN then calculate the threat of the selected hostile unit toward the selected friendly unit and add it to threat index of the selected hostile unit. After looping over all hostile unit and friendly unit, hostile units will have their summed threat index which is then compared. Hostile unit with highest threat will be selected as priority attacking unit for friendly unit. 

When moving, NN is used to calculate the location or direction to move. There are generally two conditions in “hit and run” real game play: one is hostile units are advancing toward friendly unit so that the unit should move away from hostile unit and the other is hostile unit are retreating where friendly unit should chase. Although, in this mini game, hostile units are in auto attack (attack no matter what) so only attack and move away property is trained in this project. Location of move is calculated based on multiple factor: general location of friendly unit, general location of hostile unit, friendly unit health and hostile unit health. Single decision is calculated based on this variable to determine to advance toward the enemy or retreat.

# Stages of combats and NN scores:

“Hit and Run” is designed (instead of trained) in this agent. Before the first engagement of hostile unit, friendly unit will only have attack command to ensure the initiation of combat. This stage is named Stage_1 in my code. After the first contact of friendly unit and hostile unit (damage done on hostile unit), the agent turns to Stage_2 where friendly unit will take “Attack-Move-Attack-Move” as the “Hit and Run” indicated. After some round of combat, either friendly unit or hostile unit will be very low (only one unit left). As soon as there are only one friendly unit or one hostile unit left (Stage_3), the agent will record the current score by calculating the number of hostile unit and friendly unit on the field. If friendly is expected to win (friendly unit have more hit point), the score will add up. If friendly is expected to loss, the score will be stored as the score of introduced NN. After the score is recorded, the agent turns to Stage_4 where the agent wait for the combat to finish and ready for map reset. 

# Result and discussion:

Training of NN is initiated by using random number for model parameter. In each generation, 10 candidates was used where each candidate have run ten time and the mean score of these ten combats will be the score of current agent. So, for each generation, overall 100 combats will be executed. After all combats of this generation finished, candidates will be compared and processed to form a new set of candidate based on evolution algorithm method. The newly generated candidates will go through another round of combats and this process continuous until some satisfactory end condition satisfied.

Based on Attack Zergling and Baneling Example, the training of NN is successful. One don’t require many trials to find the optimal parameter for the NN. This is the short video I captured, it only demonstrate 10 sample runs and they are pretty good. https://youtu.be/yVGeUCtSgj8. Average score of this current agent is around 550 while the highest score is 630, which is quite close to human players. After 30 generations of training, highest mean score reached for a single agent is about 580 where the highest score observe for such agent is 690. 

In occasion, the map itself will bug out (marine still in the middle while no zerglings and banelings spawning. I think this is some issue on map side and the average score of this agent could be higher is this issue can be fixed. Another issue is with score recording. In some rare case, for example the 32 recorded in the video, the program will return incorrect number of current episode. I’ll attempt to fix this later.

One can plot the mean score of each candidate in every generation over number of generations. Because first generation is only some random guess, its performance is bad. One can observe the increase of candidate score after first couple of generations while the highest mean score of each generation become stable around 580 after 15 generations. This highest score, I believe, is the limitation that introduced by the rule base where for such rules, it is impossible to exceed this limitation no matter the selection of NN parameter. Another point to notice is four peculiar generations, generation 3, 4, 12 and 13 where a drop of score is observed. This is due to the selection mechanism of evolution algorithm. Because the selection of elite candidate of each generation is based on roulette method which only guarantee highest score candidate only have highest possibility to be selected as elite, it is possible that any candidate, even the one with lowest score, to be selected as elite. 

Overall, the agent combine both neural network and rule base works well in this mini game. Evolution Algorithm identified the optimal model parameter in small amount of trials. The agent designed have superior performance than the AI designed in original publication.
